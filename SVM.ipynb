{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine\n",
    "\n",
    "### Concept of margins\n",
    "\n",
    "&emsp; Here we will consider a binary classification problem with labels $y\\in\\{-1,1\\}$ and employ the following simple classification function\n",
    "\n",
    "$g(z)=\\left\\{ \n",
    "\\begin{array}{ll} \n",
    "+1,& z\\geq 0 \\\\   \n",
    "-1, & z<0\n",
    "\\end{array}\\right. ,$\n",
    "\n",
    "where having vector of features $x$ we introduce the notation $z(x)=w^T x + b$ with the vector of parameters $w$ and $b\\in \\mathbb{R}$. Thus we will consider the classifier $h_{w,b}(x)=g(w^T x + b)$.\n",
    "\n",
    "&emsp; Let us first introduce a **functional margin** \n",
    "\n",
    "$\\hat{\\gamma}^{(i)}=(w^T x^{(i)} + b)y^{(i)}$, \n",
    "\n",
    "\n",
    "where $(x^{(i)}, y^{(i)})$ is a training example labeled by $i$. Note that together with $h_{w,b}(x^{(i)})$, the functional margin in some sense represents the confidence of our prediction. That is, when $y^{(i)}=+1$ and $w^T x^{(i)} + b\\gg 0$ then $\\hat{\\gamma}^{(i)}\\gg 0$ and at the same time we are deep in the $+1$ regime of our classifier $h_{w,b}(x)$. Similarly for $y^{(i)}=-1$ and $w^T x^{(i)} + b\\ll 0$, $\\hat{\\gamma}^{(i)}\\gg 0$ and $h_{w,b}(x^{(i)})=-1$. Nevertheless, while $h_{w,b}(x^{(i)})$ is unaffected when rescaling $(w, b) \\rightarrow (\\alpha w, \\alpha b), \\alpha\\in \\mathbb{R}_+$, the value of functional margin $\\hat{\\gamma}^{(i)}\\rightarrow \\alpha \\hat{\\gamma}^{(i)}$. Therefore, the functional margin can be arbitarily large. This issue can be overcome by *normalizing* $(w, b)\\rightarrow (w/||w||, b/||w||)$. Such a normalized margin has a very neat geometrical interpretation that I briefly describe later. Now it is worth mentioning that for a given training set $S=\\{(x^{(i)},y^{(i)}):i =1,2,\\ldots,M_S\\}$ one defines a function margin as\n",
    "\n",
    "$\\hat{\\gamma}=\\mathrm{min}_i \\,\\hat{\\gamma}^{(i)}$.\n",
    "\n",
    "&emsp; One may ask a smart question if there is some strict relation between the direction of $w$ vector and the decision boundary given by the equality $w^T x + b =0$. To find an answer we can consider two points characterized by vectors $x$ and $x'=x+\\delta$ belonging to the decision boundary. Assume now that these points are very close so that locally the part of the decision boundary connecting them is a straight line along the vector $\\delta$. Thus, due to relations $w^T x + b =0$ \\& $w^T x' + b =0$ one finds\n",
    "\n",
    "$w^T\\delta =0$.\n",
    "\n",
    "This result leads to the conclusion that the vector $w$ is always perpendicular to the decision boundary.\n",
    "\n",
    "&emsp; Let us now consider a training sample $(x_A,y_A)$ with $x_A$ representing a point that does not belong to the decision boundary. The vector $x_A$ can be decomposed as follows $x_A=x_{db}+x_\\perp$, where $w^T x_{db}+b=0$ and $x_\\perp$ is perpendicular to the decision boundary in the point characterized by $x_{db}$. By computing the normalized functional margin at $x_A$ we find\n",
    "\n",
    "$\\gamma_A = \\frac{y_A}{||w||}(w^T x_A +b)=\\frac{y_A}{||w||}w^T x_\\perp +\\frac{y_A}{||w||}(w^T x_{db}+b)=\\frac{y_A}{||w||}w^T x_\\perp$.\n",
    "\n",
    "Noting that $w \\parallel x_\\perp$ and that $w/||w||$ has a unit length one immediately recognizes $\\gamma_A$ as a minimal distance between point $A$ and the decision boundary. Therefore the distance\n",
    "\n",
    "$\\gamma^{(i)}=\\frac{y^{(i)}}{||w||}(w^T x^{(i)}+b)$,\n",
    "\n",
    "is called the **geometric margin**. As before for the training set $S$ we will be interested in the smallest of the geometric margins\n",
    "\n",
    "$\\gamma=\\mathrm{min}_i \\gamma^{(i)}$.\n",
    "\n",
    "\n",
    "### Lagrange duality\n",
    "\n",
    "&emsp; Let us consider the followig optimization problem\n",
    "\n",
    "$\\mathrm{min}_w f(w) \\quad$ s.t. $\\quad g_{i=1,2,\\ldots,k}(w)\\leq 0 \\quad \\& \\quad h_{i=1,2,\\ldots,l}(w)=0$,\n",
    "\n",
    "that will be called **primal**. Employing Lagrange multipliers method one can define\n",
    "\n",
    "$\\displaystyle{\\mathcal{L}(w,\\alpha, \\beta) = f(w) +\\sum_{i=1}^k \\alpha_i g_i(w) +\\sum_{i=1}^l \\beta_i h_i(w)}$,\n",
    "\n",
    "with $\\alpha_i$ and $\\beta_i$ Lagrange multipliers. Intoduce $\\theta_P(w)=\\max_{\\beta,\\alpha\\,:\\,\\alpha\\geq0}\\mathcal{L}(w,\\alpha, \\beta)$ with the restriction that $\\theta_P(w)=\\infty$ if $ g_{i}(w)> 0$ or $h_{i}(w)\\neq0$ for some $i$. On the other hand for $w$ satisfying primal constraints $\\theta_P(w)=f(w)$. Thus, our minimization problem is equivalent to $\\min_w \\theta_P(w) = p^*$ where $p^*$ is the optimal value of the primal problem. Similarly we can introduce **dual** optimization problem by defining $\\theta_D(\\alpha,\\beta)=\\min_w \\mathcal{L}(w,\\alpha,\\beta)$ where $\\max_{\\beta,\\alpha\\,:\\,\\alpha\\geq0} \\theta_D(\\alpha,\\beta) =d^*$ with $d^*$ being the optimal value for the dual optimization problem. Due to the fact that \"max min\" $\\leq$ \"min max\" one immediately obtains $d^*\\leq p^*$.\n",
    "\n",
    "&emsp; If now $f$ and $g_i$'s are convex, $h_i$'s are affine ($h_i(w)=a_i^Tw+b_i$) and $\\exists_w \\forall_i \\, : \\, g_i(w)<0$, then there exist $w^*,\\alpha^*,\\beta^*$, where $w^*$ and $\\{\\alpha^*,\\beta^*\\}$ are solutions of primal and dual optimization problems, respectively. Additionaly, $p^*=d^*=\\mathcal{L}(w^*,\\alpha^*,\\beta^*)$ and $w^*,\\alpha^*,\\beta^*$ satisfy Karush-Kuhn-Tucker (KKT) conditions [https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions]. Thus, we can solve the dual problem instead of the primal one.\n",
    "\n",
    "### The optimal margin classifier\n",
    "\n",
    "&emsp; We would like to determine a decision boundary for a given training set. For this purpose we would like to use the geometric margin separating the -1 and +1 training examples basing on the Euclidean distance. The original optimization problem \n",
    "\n",
    "$\\max_{\\gamma, w, b} \\gamma \\quad \\text{s.t} \\quad \\forall_i \\, : \\, \\gamma^{(i)}(w^T x^{(i)}+b)\\geq \\gamma \\quad \\& \\quad ||w||=1$,\n",
    "\n",
    "possesses a problematic (non-convex) constraint $||w||=1$ that we want to ged rid of. First, we can switch from the geometric to functional margin ($\\gamma \\rightarrow \\hat{\\gamma}$) and maximize $\\hat{\\gamma}/||w||$ instead of $\\gamma$. Second, using the freedom of scaling  $(w, b) \\rightarrow (\\alpha w, \\alpha b)$ one can impose the (scaling) constraint $\\hat{\\gamma}=1$ so that the problem reduces to\n",
    "\n",
    "$\\max_{ w, b} \\frac{1}{||w||} \\quad \\text{s.t} \\quad \\forall_i \\, : \\, \\gamma^{(i)}(w^T x^{(i)}+b)\\geq 1.$\n",
    "\n",
    "By noting that maximization of $1/||w||$ is equivalent to minimization of  $||w||^2$ we simplify the problem to the form\n",
    "\n",
    "$\\min_{ w, b} \\frac{1}{2}||w||^2 \\quad \\text{s.t} \\quad \\forall_i \\, : \\, \\gamma^{(i)}(w^T x^{(i)}+b)\\geq 1,$\n",
    "\n",
    "where we need to minimize convex quadratic function with linear constraints. This will be our *primal* problem with \n",
    "\n",
    "$g_i(w)=-y^{(i)}(w^T x^{(i)}+b)+1\\leq 0,$\n",
    "\n",
    "which is affine and then convex and $h_i(w)$ is identically 0. Thus, according to the previous discussion we should have $p^*=d^*$ and KKT conditions satisfied, where one of them reads $\\alpha_i^* g_i(w^*)=0$ for $i=1,2,\\ldots,k$. This, together with the constraint $g_i(w)\\leq0$ means that the multipliers $\\alpha_i^*>0$  only for the points $x^{(i)}$ corresponding to $g_i(w^*)=0 \\Longrightarrow y^{(i)}(w^{*T} x^{(i)}+b)=1$, i.e. for the training examples with the smallest margins. These points are called **support vectors**. For other points (usually the vast majority of points) $\\alpha_i^*=0$.\n",
    "\n",
    "&emsp; Ok, let us massage our lagrangian \n",
    "\n",
    "$\\mathcal{L}(w,b, \\alpha)=\\frac{1}{2}||w||^2-\\sum_i \\alpha_i [y^{(i)}(w^T x^{(i)}+b)-1]$,\n",
    "\n",
    "where the dual problem requires minimization of $\\mathcal{L}$ with respect to $w$ and $b$\n",
    "\n",
    "$\\nabla_w \\mathcal{L}=0 \\Longrightarrow w=\\sum_{i}\\alpha_i y^{(i)} x^{(i)} \\qquad \\& \\qquad \\partial_b \\mathcal{L}=0 \\Longrightarrow \\sum_{i}\\alpha_i y^{(i)}=0$,\n",
    "\n",
    "which leads to the dual optimization problem in the form\n",
    "\n",
    "$\\max_{\\alpha }\\,  W(\\alpha)=\\sum_{i}\\alpha - \\frac{1}{2}\\sum_{i,j}y^{(i)}y^{(j)}\\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle  \\quad \\text{s.t.} \\quad  \\forall_i \\, : \\, \\alpha_i\\geq 0 \\quad \\& \\quad \\sum_{i}\\alpha_i y^{(i)}=0.$\n",
    "\n",
    "Note that here we have the the inner product $\\langle x^{(i)}, x^{(j)} \\rangle =  [x^{(i)}]^T x^{(j)}$ that will be crucial for the formulation of the algorithm later. It is also worth noting that $w^T x+b = \\sum_{i}\\alpha_i y^{(i)}\\langle x^{(i)}, x\\rangle+b$ due to the equality $w=\\sum_{i}\\alpha^i y^{(i)} x^{(i)}$.\n",
    "\n",
    "&emsp; If we determined the optimal $\\alpha_i^*$'s then the original parameters can be found by the relations \n",
    "\n",
    "$w^*=\\sum_{i}\\alpha^{*}_i y^{(i)} x^{(i)} \\qquad \\& \\qquad  b=-[\\max_{i:y^{(i)}=-1}w^{*T}x^{(i)}+\\min_{i:y^{(i)}=1}w^{*T}x^{(i)}]/2$,\n",
    "\n",
    "where the second equality comes from the decision boundary constraint for support vectors, i.e. $y^{(i)}(w^{*T}x^{(i)}+b)=1 \\stackrel{y^{(i)}=\\pm1}\\Longrightarrow (w^{*T}x^{(i)}+b)=y^{(i)}$.\n",
    "So the closest negative ($y^{(i)}=-1$) and positive ($y^{(i)}=1$) examples correspond to $\\max_{i:y^{(i)}=-1}w^{*T}x^{(i)} +b = -1  \\,\\, \\& \\,\\,  \\min_{i:y^{(i)}=1}w^{*T}x^{(i)}+b=1$. The result is found when adding both relations and determining $b$.\n",
    "\n",
    "### Kernels\n",
    "\n",
    "&emsp; Our reformulated problem depends now on the attributes vectors $x^{(i)}$ only through the inner products $\\langle x^{(i)}, x^{(j)} \\rangle$ measuring the similarity between $x^{(i)}$ and $x^{(j)}$. To incorporate higher dimensional dependences between attributes one introduces some **feature mapping** $x\\rightarrow \\phi(x)$ replacing the inner products $\\langle x, x' \\rangle$ with $\\langle \\phi(x), \\phi(x') \\rangle = \\phi(x)^T \\phi(x')=K(x,x')$, where $K$ is called **kernel** and to some extend measures the similarity between $\\phi(x)$ and $\\phi(x')$. For example, $K(x,x')=(x^T x')^2=\\sum_{i,j}(x_i x'_i)(x_j x_j')=\\sum_{i,j}(x_i x_j)(x'_i x'_j)$, where $x_i x_j$ are elements of $\\phi(x)$. In such a case if $\\mathrm{len}(x) = n$ then computing elements of $\\phi(x)$ requires $O(n^2)$ time, while determing $K(x,x')$ takes $O(n)$ only. More generally one can consider the kernel $K(x,x') = (x^T x'+b)^d$ which determination demands $O(n)$ time only.\n",
    "\n",
    "&emsp; Given the intuition where $K(x,x')$ represents a measure of similarity between $\\phi(x)$ and $\\phi(x')$ and thus to some extend similarity between $x$ and $x'$ one can consider kernels being functions of $||x-x' ||$. The most natural choice would be $K(x,x')=\\exp\\left[-\\frac{1}{2}||x-x'||^2/\\sigma^2\\right]$ (**Gaussian kernel**), which corresponds to an infinite dimensional feature mapping $\\phi$.\n",
    "\n",
    "&emsp; Introducing the kernel trick our high-dimensional decision boundary is given by \n",
    "\n",
    "$w^T x+b = \\sum_{i}\\alpha_i y^{(i)}\\langle x^{(i)}, x\\rangle+b =0 \\stackrel{x\\rightarrow \\phi(x)}{\\Longrightarrow} \\sum_{i}\\alpha_i y^{(i)}K(x^{(i)},x)+b =0, $\n",
    "\n",
    "where now looking at the closest negative ($y^{(i)}=-1$) and positive ($y^{(i)}=1$) examples we have \n",
    "\n",
    "$\\max_{i:y^{(i)}=-1}\\sum_{j}\\alpha_i y^{(j)}K(x^{(j)},x^{(i)})+b = -1  \\,\\, \\& \\,\\,  \\min_{i:y^{(i)}=1}\\sum_{j}\\alpha_i y^{(j)}K(x^{(j)},x^{(i)})+b=1$.\n",
    "\n",
    "Therefore $b^*=-\\frac{1}{2}\\left[\\max_{i:y^{(i)}=-1}\\sum_{j}\\alpha_i y^{(j)}K(x^{(j)},x^{(i)})+ \\min_{i:y^{(i)}=1}\\sum_{j}\\alpha_i y^{(j)}K(x^{(j)},x^{(i)})\\right]$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "&emsp; Let us consider set of $m$ points $\\{x^{(1)},\\ldots, x^{(m)}\\}$ and the kernel matrix $\\mathbb{K}_{ij}=K(x^{(i)},x^{(j)})=\\phi(x^{(i)})^T \\phi(x^{(j)})=\\phi(x^{(j)})^T \\phi(x^{(i)})$ where the symmetry relation $\\mathbb{K}_{ij}=\\mathbb{K}_{ji}$ is required for $K$ to be valid kernel (valid kernel: there exists some mapping $\\phi$ corresponding to $K$). Now, for any $z$ one finds\n",
    "\n",
    "$z^T \\mathbb{K} z=\\sum_{i,j}z_i \\mathbb{K}_{ij} z_j = \\sum_{i,j,k} z_i [\\phi(x)]_k [\\phi(x)]_k z_j = \\sum_{k} \\left(\\sum_i[\\phi(x)]_k z_i  \\right)^2\\geq 0,$\n",
    "\n",
    "and thus $\\mathbb{K}$ has to be positive semi-definite. According to *Mercer theorem*, for $K$ to be valid it is necessary and sufficient that $\\mathbb{K}$ is symmetric and semi-definite.\n",
    "\n",
    "### Regularization and nonseparable cases\n",
    "\n",
    "Note that the decision boundary can change dramatically when we add even a single outlier. Therefore, one regularizes the algorithm making it less sensitive to outliers by allowing the scaled functional margin to be less than 1, i.e.,\n",
    "\n",
    "$\\min_{ w, b} \\frac{1}{2}||w||^2+C\\sum_{i}\\xi_i \\quad \\text{s.t} \\quad \\forall_i \\, : \\, \\gamma^{(i)}(w^T x^{(i)}+b)\\geq 1-\\xi_i \\quad \\& \\quad \\xi_i\\geq1.$\n",
    "\n",
    "In such a case our lagrangian reads ($\\alpha_i, r_i\\geq 0$)\n",
    "\n",
    "$\\mathcal{L}(w,b,\\xi, \\alpha,r)=\\frac{1}{2}||w||^2+C\\sum_{i}\\xi_i-\\sum_i \\alpha_i [y^{(i)}(w^T x^{(i)}+b)-1+\\xi_i] -  \\sum_ir_i \\xi_i$,\n",
    "\n",
    "with the dual problem ($\\partial_{\\xi_j}\\mathcal{L}=0\\Longrightarrow C-\\alpha_j=r_j\\geq 0$)\n",
    "\n",
    "\n",
    "$\\max_{\\alpha }\\,  W(\\alpha)=\\sum_{i}\\alpha - \\frac{1}{2}\\sum_{i,j}y^{(i)}y^{(j)}\\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle  \\quad \\text{s.t.} \\quad  \\forall_i \\, : \\, 0\\leq\\alpha_i\\leq C \\quad \\& \\quad \\sum_{i}\\alpha_i y^{(i)}=0.$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
